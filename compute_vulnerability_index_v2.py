import os
import numpy as np
import pandas as pd

from sklearn.impute import KNNImputer
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans

# === CONFIG ===
BASE_DIR = "/content/drive/MyDrive/AgriVulnAI"
INPUT_CSV = os.path.join(BASE_DIR, "data/processed/ground_truth_with_predictions.csv")
OUTPUT_CSV = os.path.join(BASE_DIR, "data/processed/ground_truth_with_predictions_v2.csv")


def main():
    print(os.getcwd())
    os.chdir(BASE_DIR)
    print(os.getcwd())

    print(f"ðŸ“‚ Input:  {INPUT_CSV}")
    print(f"ðŸ“‚ Output: {OUTPUT_CSV}")

    if not os.path.exists(INPUT_CSV):
        raise FileNotFoundError(f"Nu gÄƒsesc fiÈ™ierul de intrare: {INPUT_CSV}")

    # ---------------------------------------------------------
    # 1. Load data
    # ---------------------------------------------------------
    df = pd.read_csv(INPUT_CSV)
    print(f"Loaded {len(df):,} rows.")
    print("Columns:", list(df.columns))

    # ---------------------------------------------------------
    # 2. Select feature columns for Vulnerability Index v2
    #    (include exact features pe care vrei sÄƒ le consideri)
    # ---------------------------------------------------------
    feature_cols_for_impute = [
        "NDVI_2023-03-01",
        "EVI_2023-03-01",
        "CHIRPS_precip_2023-03-01",
        "PM25_2023-03-01",
        "CAMS_PM25_JanMar2023",
        "Water_occurrence",
        "WorldPop_2020",
    ]

    # VerificÄƒm ce coloane chiar existÄƒ Ã®n CSV
    available_features = [c for c in feature_cols_for_impute if c in df.columns]
    missing_features = [c for c in feature_cols_for_impute if c not in df.columns]

    print("\nâœ… Feature columns available:")
    for c in available_features:
        print(f"  - {c}")

    if missing_features:
        print("\nâš ï¸ Missing feature columns (vor fi ignorate):")
        for c in missing_features:
            print(f"  - {c}")

    if not available_features:
        raise ValueError("Nu existÄƒ niciuna dintre coloanele aÈ™teptate pentru index v2.")

    # LuÄƒm doar coloanele disponibile pentru imputare
    X = df[available_features].copy()

    # ---------------------------------------------------------
    # 3. KNN Imputer (varianta corectÄƒ, cu scaling)
    #    - facem standardizare (mean/std) pe fiecare col, ignorÃ¢nd NaN
    #    - aplicÄƒm KNNImputer Ã®n spaÈ›iul standardizat
    #    - reconstruim valorile Ã®napoi Ã®n unitÄƒÈ›ile originale
    # ---------------------------------------------------------
    print("\nðŸ“Š Missing values BEFORE KNN imputer:")
    print(X.isna().sum())

    means = X.mean(skipna=True)
    stds = X.std(skipna=True, ddof=0)

    # Ca sÄƒ evitÄƒm Ã®mpÄƒrÈ›irea la 0
    stds_replaced = stds.replace(0, 1.0)

    X_scaled = (X - means) / stds_replaced

    imputer = KNNImputer(
        n_neighbors=5,  # poÈ›i schimba Ã®n 3/4/6 dacÄƒ vrei sÄƒ experimentezi
        weights="distance"
    )

    X_scaled_imputed = imputer.fit_transform(X_scaled)

    # ÃŽnapoi Ã®n unitÄƒÈ›i originale
    X_imputed = (X_scaled_imputed * stds_replaced.values) + means.values

    X_imputed_df = pd.DataFrame(X_imputed, columns=available_features, index=df.index)

    print("\nðŸ“Š Missing values AFTER KNN imputer (ar trebui sÄƒ fie 0):")
    print(X_imputed_df.isna().sum())

    # Suprascriem Ã®n df valorile imputate
    for c in available_features:
        df[c] = X_imputed_df[c]

    # ---------------------------------------------------------
    # 4. Construim componentele de risc È™i Vulnerability Index v2
    # ---------------------------------------------------------
    # Vom normaliza componentele la [0,1] cu MinMaxScaler
    scaler = MinMaxScaler()

    # Vegetation stress (NDVI + EVI) â€“ stres = 1 - green
    veg_cols = [c for c in ["NDVI_2023-03-01", "EVI_2023-03-01"] if c in df.columns]
    if veg_cols:
        veg_scaled = scaler.fit_transform(df[veg_cols])
        veg_mean = veg_scaled.mean(axis=1)
        veg_stress = 1.0 - veg_mean  # valori mari => vegetaÈ›ie mai slabÄƒ
    else:
        veg_stress = np.zeros(len(df))

    # Rainfall deficit (CHIRPS) â€“ stres = 1 - precip
    if "CHIRPS_precip_2023-03-01" in df.columns:
        rain_scaled = scaler.fit_transform(df[["CHIRPS_precip_2023-03-01"]])[:, 0]
        rainfall_deficit = 1.0 - rain_scaled
    else:
        rainfall_deficit = np.zeros(len(df))

    # Pollution (PM25 + CAMS)
    poll_cols = []
    if "PM25_2023-03-01" in df.columns:
        poll_cols.append("PM25_2023-03-01")
    if "CAMS_PM25_JanMar2023" in df.columns:
        poll_cols.append("CAMS_PM25_JanMar2023")

    if poll_cols:
        poll_scaled = scaler.fit_transform(df[poll_cols])
        pollution_risk = poll_scaled.mean(axis=1)
    else:
        pollution_risk = np.zeros(len(df))

    # Exposure (WorldPop_2020)
    if "WorldPop_2020" in df.columns:
        pop_scaled = scaler.fit_transform(df[["WorldPop_2020"]])[:, 0]
        exposure_risk = pop_scaled
    else:
        exposure_risk = np.zeros(len(df))

    # Optional: includem pred_class ca indicator de risc (0â€“10)
    # ÃŽl normalizÄƒm la [0,1] dacÄƒ existÄƒ
    if "pred_class" in df.columns:
        pred_scaled = scaler.fit_transform(df[["pred_class"]])[:, 0]
        model_risk = pred_scaled
    else:
        model_risk = np.zeros(len(df))

    # Stack components
    components = np.vstack([
        veg_stress,
        rainfall_deficit,
        pollution_risk,
        exposure_risk,
        model_risk,
    ]).T  # shape (n_samples, n_components)

    # Vulnerability Index v2 = media componentelor (poÈ›i ajusta weighting dacÄƒ vrei)
    vuln_index_v2 = components.mean(axis=1)

    # NormalizÄƒm din nou la [0,1] ca sÄƒ fim siguri
    vuln_index_v2_scaled = MinMaxScaler().fit_transform(vuln_index_v2.reshape(-1, 1))[:, 0]

    df["vuln_index_v2"] = vuln_index_v2_scaled

    print("\nðŸ“ˆ Vulnerability Index v2 stats (0â€“1):")
    print(df["vuln_index_v2"].describe())

    # ---------------------------------------------------------
    # 5. Clustering (KMeans) Ã®n funcÈ›ie de componentele de risc
    # ---------------------------------------------------------
    print("\nðŸ¤– Running KMeans with K=4 on risk components...")

    kmeans = KMeans(
        n_clusters=4,
        random_state=42,
        n_init=10
    )

    clusters_raw = kmeans.fit_predict(components)

    # OrdonaÈ›i clusterele de la low -> high dupÄƒ media indexului v2
    cluster_means = {}
    for c in range(4):
        cluster_means[c] = df.loc[clusters_raw == c, "vuln_index_v2"].mean()

    # sortÄƒm dupÄƒ mean index
    sorted_clusters = sorted(cluster_means.items(), key=lambda x: x[1])  # list of (raw_label, mean)

    print("\nðŸ“Š Cluster means (ordonate lowâ†’high):")
    for rank, (raw_label, mean_val) in enumerate(sorted_clusters):
        n_points = (clusters_raw == raw_label).sum()
        print(f"  Cluster {rank} (raw={raw_label}): mean index={mean_val:.3f}, n={n_points}")

    # map raw cluster -> ordered 0..3
    raw_to_ordered = {raw: rank for rank, (raw, _) in enumerate(sorted_clusters)}
    vuln_cluster_v2 = np.array([raw_to_ordered[c] for c in clusters_raw])

    df["vuln_cluster_v2"] = vuln_cluster_v2

    # ---------------------------------------------------------
    # 6. Save
    # ---------------------------------------------------------
    df.to_csv(OUTPUT_CSV, index=False)

    print(f"\nâœ… Saved with v2 index & clusters to:")
    print(f"   {OUTPUT_CSV}")
    print("Columns now include: 'vuln_index_v2', 'vuln_cluster_v2'")


if __name__ == "__main__":
    main()
