"""
compute_vulnerability_index_v2.py

ConstruieÈ™te un Vulnerability Index v2 pentru punctele din
ground_truth_with_predictions.csv, folosind:

- indici satelitari (NDVI/EVI/CHIRPS)
- poluare aer (PM2.5, CAMS_PM25)
- apÄƒ de suprafaÈ›Äƒ (Water_occ)
- densitate populaÈ›ie (WorldPop_2020)
- clasa prezisÄƒ (pred_class)

Output:
- data/processed/ground_truth_with_predictions_v2.csv
  cu coloane noi:
    - vuln_index_v2    (continuu, 0â€“1)
    - vuln_cluster_v2  (cluster ordonat 0=low â€¦ K-1=extreme)
"""

from pathlib import Path

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler

try:
    # dacÄƒ existÄƒ config.py Ã®n proiect, Ã®l folosim pentru path-uri
    from config import PROCESSED_DIR, RESULTS_DIR
    BASE_PROCESSED_DIR = Path(PROCESSED_DIR)
except ImportError:
    # fallback: rulÄƒm direct din folderul scriptului
    BASE_DIR = Path(__file__).resolve().parent
    BASE_PROCESSED_DIR = BASE_DIR / "data" / "processed"

IN_PATH = BASE_PROCESSED_DIR / "ground_truth_with_predictions.csv"
OUT_PATH = BASE_PROCESSED_DIR / "ground_truth_with_predictions_v2.csv"

print(f"ğŸ“‚ Input:  {IN_PATH}")
print(f"ğŸ“‚ Output: {OUT_PATH}")

# -------------------------
# 1. Load data
# -------------------------
if not IN_PATH.exists():
    raise FileNotFoundError(f"Nu gÄƒsesc fiÈ™ierul de input: {IN_PATH}")

df = pd.read_csv(IN_PATH)
print(f"Loaded {len(df):,} rows.")
print("Columns:", list(df.columns))

# -------------------------
# 2. Select feature columns
# -------------------------
feature_cols = [
    "NDVI_2023-03-01",
    "EVI_2023-03-01",
    "CHIRPS_precip_2023-03-01",
    "PM25_2023-03-01",
    "CAMS_PM25_JanMar2023",
    "Water_occurrence",
    "WorldPop_2020",
    "pred_class",
]

available_cols = [c for c in feature_cols if c in df.columns]
missing_cols = [c for c in feature_cols if c not in df.columns]

print("\nâœ… Feature columns available:")
for c in available_cols:
    print("  -", c)

if missing_cols:
    print("\nâš ï¸ Missing feature columns (vor fi ignorate):")
    for c in missing_cols:
        print("  -", c)

if not available_cols:
    raise ValueError("Nu existÄƒ niciuna din coloanele dorite pentru feature-uri.")

X = df[available_cols].copy()

# -------------------------
# 3. Handle missing values
# -------------------------
numeric_cols = X.columns
X[numeric_cols] = X[numeric_cols].astype(float)

# Ã®nlocuim NaN cu medianÄƒ pe fiecare coloanÄƒ
medians = X.median(numeric_only=True)
X[numeric_cols] = X[numeric_cols].fillna(medians)

print("\nğŸ“Š Missing values (dupÄƒ umplere):")
print(X.isna().sum())

# -------------------------
# 4. Normalize (0â€“1) & convert to "risk" components
# -------------------------
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X.values)
X_scaled_df = pd.DataFrame(X_scaled, columns=available_cols, index=df.index)

# coloane unde valorile mari = condiÈ›ii bune (deci risc mai mic)
good_cols = {
    "NDVI_2023-03-01",
    "EVI_2023-03-01",
    "CHIRPS_precip_2023-03-01",
    "Water_occurrence",
}

# construim matricea de risc: 1 - bun, respectiv direct
risk_components = {}
for col in available_cols:
    if col in good_cols:
        # mai mare = mai bine -> risc = 1 - scaled
        risk_components[col] = 1.0 - X_scaled_df[col]
    else:
        # mai mare = mai rÄƒu -> risc = scaled
        risk_components[col] = X_scaled_df[col]

risk_df = pd.DataFrame(risk_components, index=df.index)

# continuous index: media componentelor de risc
vuln_index_v2 = risk_df.mean(axis=1)

# normalizÄƒm din nou Ã®n [0,1] (de siguranÈ›Äƒ)
v_min, v_max = vuln_index_v2.min(), vuln_index_v2.max()
if v_max > v_min:
    vuln_index_v2_norm = (vuln_index_v2 - v_min) / (v_max - v_min)
else:
    vuln_index_v2_norm = pd.Series(0.5, index=df.index)

df["vuln_index_v2"] = vuln_index_v2_norm

print("\nğŸ“ˆ Vulnerability Index v2 stats (0â€“1):")
print(df["vuln_index_v2"].describe())

# -------------------------
# 5. Clustering (KMeans)
# -------------------------
K = 4  # poÈ›i schimba Ã®n 3/5/6 etc.
print(f"\nğŸ¤– Running KMeans with K={K} on risk components...")

kmeans = KMeans(n_clusters=K, random_state=42, n_init="auto")
cluster_raw = kmeans.fit_predict(risk_df.values)

# ordonÄƒm clusterele dupÄƒ media indexului, ca sÄƒ avem 0=low ... K-1=high
cluster_means = {}
for k in range(K):
    cluster_means[k] = vuln_index_v2_norm[cluster_raw == k].mean()

# sortÄƒm cluster labels dupÄƒ medie
sorted_clusters = sorted(cluster_means.items(), key=lambda x: x[1])  # (cluster_label, mean)
old_to_new = {old: new for new, (old, _) in enumerate(sorted_clusters)}

cluster_ordered = np.vectorize(old_to_new.get)(cluster_raw)

df["vuln_cluster_v2"] = cluster_ordered.astype(int)

print("\nğŸ“Š Cluster means (ordonate lowâ†’high):")
for new_label, (old_label, mean_val) in enumerate(sorted_clusters):
    count = (cluster_ordered == new_label).sum()
    print(f"  Cluster {new_label} (raw={old_label}): mean index={mean_val:.3f}, n={count}")

# -------------------------
# 6. Save output
# -------------------------
OUT_PATH.parent.mkdir(parents=True, exist_ok=True)
df.to_csv(OUT_PATH, index=False)

print(f"\nâœ… Saved with v2 index & clusters to:\n   {OUT_PATH}")
print("Columns now include: 'vuln_index_v2', 'vuln_cluster_v2'")
